{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pertama-tama <i>import</i> dulu <i>library</i>-nya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perlu diingat TextBlob merupakan <i>library</i> NLP berbahasa inggirs jadi kalimat yang saya gunakan pada tutorial ini adalah bahasa inggris. Akan saya jelaskan satu per satu biar ndak ada kesalahpahaman di antara kita heheheh. <i>Let's get started</i>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"from textblob\" itu kita memanggil library textblob namun tidak seluruh bagian dari library tersebut terpanggil karena kita hanya (\"import TextBlob\") menggunakan fungsi TextBlob yang tersedia di library textblob. Paham'kan? misalnya begini saya punya library bernama luasLingkaran dan di dalam library tersebut saya memiliki dua fungsi yakni fungsi hitungLuas dan phi. Ketika saya hanya ingin menggunakan fungsi phi saja tanpa melibatkan fungsi hitungLuas saya hanya cukup menulis seperti ini \"from luasLingkaran import phi\". Saya yakin kalian sudah paham (: \"as\" sendiri itu untuk mengaliaskan atau bisa juga untuk menyingkat kata-kata agar kode lebih efisien. Sebenarnya istilah library itu sama dengan framework, sama dengan modul, sama dengan depedensi, atau apalah kalian menyebutnya heheheh pada tutorial ini saya menggunakan istilah library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = tb(\"TextBlob is a library for natural language processing, it is easy to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TextBlob', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('library', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('easy', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('use', 'VB')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging adalah suatu proses yang memberikan label kelas kata secara otomatis pada suatu kata dalam kalimat. Fungsi \"tags\" di atas itu gunanya untuk melakukan POS tagging terhadap variabel wiki yang outputnya berupa list dari tuple-tuple yang berisi kata dan labelnya. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['textblob', 'natural language processing'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi noun_phrases gunanya untuk mengetahui <i>noun phrase</i> yang terdapat pada kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit = tb(\"Python is amazingly simple to use. What great fun!\")\n",
    "twit.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis sentimen merupakan cabang dari text mining, fokus utamanya ya buat nganalisa teks :v biasanya saya pake buat nganalisa opini dari para netizen heheheheh. Rentang dari \"subjectivity\" itu 0.0 sampai 1.0 jika mendekati 0.0 berarti sentimen tersebut sangat objektif begitu pula sebaliknya. Rentang \"polarity\" itu dari -1.0 sampai 1.0 jika diterjemahkan polarity = polaritas, artinya hal yang memiliki dua sifat yang berlawanan. Jadi kalau polarity-nya 0.0 berati sentimen tersebut bersifat netral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Complex', 'is', 'better', 'than', 'complicated', 'Flat', 'is', 'better', 'than', 'nested', 'Sparse', 'is', 'better', 'than', 'dense'] \n",
      "\n",
      "[Sentence(\"Complex is better than complicated.\"), Sentence(\"Flat is better than nested.\"), Sentence(\"Sparse is better than dense.\")]\n"
     ]
    }
   ],
   "source": [
    "zen = tb(\"Complex is better than complicated. \"\n",
    "         \"Flat is better than nested. \"\n",
    "         \"Sparse is better than dense.\")\n",
    "\n",
    "print(zen.words,\"\\n\")\n",
    "print(zen.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token-token. Keluaran pertama itu membagi per kata sedagkan keluaran ke dua itu membagi per kalimat. Sederhana'kan? Saya kasih bonus nih!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.09999999999999999, subjectivity=0.6333333333333333)\n",
      "Sentiment(polarity=0.2375, subjectivity=0.3125)\n",
      "Sentiment(polarity=0.5, subjectivity=0.5)\n"
     ]
    }
   ],
   "source": [
    "for i in zen.sentences:\n",
    "    print(i.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perubahan bentuk kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foxes', 'jump', 'over', 'the', 'lazy', 'dog'] \n",
      "\n",
      "fox \n",
      "\n",
      "dogs\n"
     ]
    }
   ],
   "source": [
    "kalimat = tb(\"foxes jump over the lazy dog\")\n",
    "print(kalimat.words,\"\\n\")\n",
    "print(kalimat.words[0].singularize(),\"\\n\")\n",
    "print(kalimat.words[5].pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fungsi singularize() gunanya untuk menjadikan objek tersebut menjadi bentuk tunggal dan fungsi pluralize() berguna untuk mengubah ke bentuk jamak. Fungsi lemmatize() sendiri berfungsi untuk mengubah kata menjadi bentuk kata dasar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "octopus\n",
      "buy\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "w = Word(\"octopi\")\n",
    "print(w.lemmatize())\n",
    "\n",
    "w = Word(\"bought\")\n",
    "print(w.lemmatize(\"v\"))  # VERB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrasi WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebelum melanjutkan pembahasan penting bagi kalian untuk mengetahui apa itu WordNet. WordNet merupakan sebuah database kamus bahasa Inggris yang dikembangkan oleh Princeton University. Perbedaan antara WordNet dengan kamus bahasa pada umumnya adalah kamus bahasa memfokuskan pada kata sedangkan WordNet memfokuskan diri kepada makna kata. Konsep utama pada WordNet adalah synsets, di mana synset merupakan kumpulan dari 1 atau lebih kata yang memiliki makna sama (dan tentunya dapat saling menggantikan dalam konteks tertentu). Harap diingat bahwa satu synset mewakili satu makna (dalam bhs Inggris = sense) yang berbeda. Misalkan, synset XXX beranggotakan apel dengan gloss nama buah yang berwarna merah. Synset YYY juga beranggotakan apel, upacara, dengan gloss upacara kemiliteran. Dari dua contoh synset di atas, kita dapat mengetahui bahwa kata ‘apel’ memiliki dua makna yang berbeda, di mana salah satunya adalah buah, dan lainnya adalah upacara. Gloss: merupakan definisi dan/atau contoh kalimat yang melengkapi keterangan suatu synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('octopus.n.01'), Synset('octopus.n.02')] \n",
      "\n",
      "[Synset('chop.v.05'), Synset('hack.v.02'), Synset('hack.v.03'), Synset('hack.v.04'), Synset('hack.v.05'), Synset('hack.v.06'), Synset('hack.v.07'), Synset('hack.v.08')]\n"
     ]
    }
   ],
   "source": [
    "from textblob.wordnet import VERB\n",
    "word = Word(\"octopus\")\n",
    "print(word.synsets,\"\\n\")\n",
    "print(Word(\"hack\").get_synsets(pos=VERB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synsets itu singkatan dari synonym sets, \"octopus\" punya dua artian dan \"hack\" punya delapan arti yang berbeda. TextBlob juga menyediakan fitur untuk mengetahui definisi atau arti dari tiap kata yang kalian masukkan lo... kayak gini nih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"an island in Indonesia to the south of Borneo; one of the world's most densely populated regions\",\n",
       " 'a beverage consisting of an infusion of ground coffee beans',\n",
       " 'a platform-independent object-oriented programming language']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"Java\").definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buat synset secara langsung :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.wordnet import Synset\n",
    "octopus = Synset('octopus.n.02')\n",
    "shrimp = Synset('shrimp.n.03')\n",
    "octopus.path_similarity(shrimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cat', 'dog', 'octopus'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = tb(\"cat dog octopus\")\n",
    "\n",
    "animals.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidak mirip dengan tokenisasi 100% tapi ya emang cuma gini aja '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembetulan Ejaan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layaknya pencarian pada Google Search Engine, TextBlob juga bisa membetulkan kata-kata yang saltik dengan fungsi correct(). Fungsi spellcheck() berguna untuk memastikan ejaan kita itu benar dan ada di dalam kamus besar bahasa inggris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "b = tb(\"I havv goood speling!\")\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fallibility', 1.0)]\n",
      "[('fallibility', 0.3333333333333333), ('capability', 0.3333333333333333), ('affability', 0.3333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "w = Word('falibility')\n",
    "print(w.spellcheck())\n",
    "\n",
    "w = Word('falability')\n",
    "print(w.spellcheck())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada keluaran ke dua terdapat tiga kemungkinan kata yang dapat digunakan sebagai pembenaran dari kata <b>falability</b> angka di sebelahnya itu menggambarkan presentase dari pembenaran kata karena ada tiga kemungkinan maka 100/3 hasilnya ya 33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mengetahui banyaknya jumlah kata dan noun phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = tb(\"We are no longer the Knights who say Ni. \"\n",
    "           \"We are now the Knights who say Ekki ekki ekki PTANG.\")\n",
    "monty.word_counts['ekki']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada yang aneh bukan? <b>ekki</b> dalam monty sebetulnya ada dua saja namun outputnya menghasilkan tiga. Ini dikarenakan <b>word_counts[]</b> TIDAK MENDUKUNG <i>case-sensitive</i> sehingga \"Ekki\" pun ikut terhitung. Selain menggunakan <b>word_counts[]</b> kalian juga dapat menggunakan <b>words.count()</b>, yang mendukung <i>case-sensitive</i>. Lihat saja ke dua contoh di bawah ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.words.count('ekki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.words.count('ekki', case_sensitive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potongan kode di bawah ini gunanya ya buat mengetahui jumlah noun phrase dalam kalimat saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.noun_phrases.count(\"TextBlob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesin Penerjemah dan Deteksi Bahasa Sederhana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "di mana kamu sekarang?\n",
      "Where are you now?\n"
     ]
    }
   ],
   "source": [
    "masukan=input()\n",
    "translator = tb(masukan)\n",
    "print(translator.translate(to=\"en\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cukup dengan fungsi translate() kalian dapat dengan mudahnya membuat program penerjemah bahasa, mantab <i>soul</i>kan? Kenapa bisa begini? Karena TextBlob pake API dari Google Translate jadinya kalo laptop kalian nggak nyambung internet ya... Nggak bakal bisa jalan programnya heheheh. Sintaks <b>to=\"en\"</b> itu berfungsi menerjemahkan kalimat masukkan menjadi bahasa inggirs, kalo kalian mau nerjemahin ke bahasa arab, en-nya kalian ganti aja jadi <b>ar</b>. Mirip seperti Google Translate TextBlob juga memiliki fitur pendeteksi bahasa<i>(yaiyalah orang pake API'nya Google Translate)</i>. Cukup pake fungsi detect_language() aja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ar'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tb(\"بسيط هو أفضل من مجمع\")\n",
    "b.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I/PRP/B-NP/O have/VBP/B-VP/O no/DT/B-NP/O idea/NN/I-NP/O with/IN/B-PP/B-PNP this/DT/B-NP/I-PNP wild/JJ/I-NP/I-PNP animals/NNS/I-NP/I-PNP ././O/O\n"
     ]
    }
   ],
   "source": [
    "b = tb(\"I have no idea with this wild animals.\")\n",
    "print(b.parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengembalikan <i>list</i> berupa <i>WordList</i> yang terdiri dari sejumlah <b>n</b> kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = tb(\"Now is better than never.\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mendapatkan awal dan akhir indeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex is better than complicated.\n",
      "---- Mulai dari indeks 0, berakhir di indeks 35 \n",
      "\n",
      "Flat is better than nested.\n",
      "---- Mulai dari indeks 36, berakhir di indeks 63 \n",
      "\n",
      "Sparse is better than dense.\n",
      "---- Mulai dari indeks 64, berakhir di indeks 92 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in zen.sentences:\n",
    "    print(s)             #tanpa saya jelaskan kalian sudah paham sendiri'kan maksud dari format(s.start, s.end) (:\n",
    "    print(\"---- Mulai dari indeks {}, berakhir di indeks {}\".format(s.start, s.end),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tambahan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di TextBlob kalian juga dapat memodifikasi <i>List</i>. Bagi yang sudah memahami konsep <i>List</i> di dalam python tentu tanpa penjelasan saya kalian paham dengan potongan-potongan kode di bawah ini. Bagi yang belum paham belajar dulu sana! :v Bahasannya panjang dan gak mungkin saya bahas di sini heheheh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Complex is better than complicated.\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen[0:35] #Menampilkan indeks ke 0 sampai ke 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLEX IS BETTER THAN COMPLICATED. FLAT IS BETTER THAN NESTED. SPARSE IS BETTER THAN DENSE.\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "print(zen.upper())       #Merubah huruf menjadi kapital, jika ingin mengecilkan gunakan fungsi lower()\n",
    "print(zen.find(\"dense\")) #Mencari posisi indeks awal dari kata \"dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "apple_blob = tb('apples')\n",
    "banana_blob = tb('bananas')\n",
    "print(apple_blob < banana_blob)  #True karena jumlah huruf pada variabel apple_blob lebih sedikit dari variabel banana_blob\n",
    "print(apple_blob == 'apples')    #SUDAH JELAS!1!1!1!1!1!1!1!1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples and bananas\n",
      "apples and bananas\n"
     ]
    }
   ],
   "source": [
    "print(apple_blob + ' and ' + banana_blob)\n",
    "print(\"{0} and {1}\".format(apple_blob, banana_blob)) #Sudah paham sendiri'kan? (:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekian dari tutorial Pengolahan Bahasa Alami dengan Python menggunakan TextBlob. Semoga bermanfaat (:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
